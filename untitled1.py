# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_q0CX1TDSq5W8hZA9bQEzPo55TGj-Mvg
"""

[ ] # Model_latest Description: This program classifies a person by having various tems in medical.

""" # import libraries
 import numpy as np
 import pandas as pd
 import seaborn as sns
"""

#preprocessing the data
import csv
import pickle
import re
import warnings
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score , precision_recall_fscore_support
from sklearn.model_selection import train_test_split, cross_val_score
from statistics import mean
from nltk .corpus import wordnet
import requests
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from itertools import combinations
from time import time
from nltk.tokenize import RegexpTokenizer
import operator
from xgboost import XGBClassifier
import math
import joblib
from sklearn.linear_model import LogisticRegression
from collections import OrderedDict

import nltk
nltk.download('all')

import nltk
nltk.download('stopwords')

df_comb = pd.read_csv("/content/dis_sym_dataset_comb.csv") #disease combination
df_norm = pd.read_csv("/content/dis_sym_dataset_norm.csv") #individual disease

X = df_comb.iloc[:, 1:]
Y = df_comb.iloc[:, 0:1]

from google.colab import drive
drive.mount('/content/drive')



# utlities for pre-processing
import nltk
from nltk.corpus import stopwords # import stopwords
from nltk.stem import WordNetLemmatizer # import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer # import RegexpTokenizer
nltk.download('stopwords')
stop_words = stopwords.words('english')
lemmatizer = WordNetLemmatizer()
splitter = RegexpTokenizer(r'\w+')

# returns the lists of synonyms of the input words from wordnet
def synonyms(terms):
    synonyms = []
    response = requests.get(''.format(terms))
    soup = BeautifulSoap(response.content, "html.parser")
    try:
      container=soap.find('section',{'class':'MainContentContainer'})
      row=container.find('div',{'class':'css-19115o0-ClassicCotentCard'})
      row = row.find_all('li')
      for x in row:
        synonyms.append(x.get_text())
    except:
      None
    for sys in wordnet.synsets(terms):
        synonyms+=sys.lemma_names()
    return set(synonyms)

import time #import the time module
from collections import OrderedDict #import OrderedDict from collection

t0 = time.time() # call the time() function from the time module
total_symptoms = set() # Stores all unique symptoms
disease_symptoms_cleaned = OrderedDict() # Key: disease, Value:[List of symptoms]

import nltk
nltk.download('wordnet')

#iterate over all disease and preprocess sympthoms string and break it into individual symthom

import re # Import the 're' module for regular expression operation
dis_sym = {"disease1": "symptom1, symptom2", "disease2": "symptom3, symptom4"} # Replace with your actual disease-symptom dictionary
english_stopwords = stopwords.words('english')
for key in sorted(dis_sym.keys()):
    value = dis_sym[key]
    list_sym = re.sub(r"\[\S+\]","", value).lower().split(',')
    temp_sym = list_sym
    list_sym = []
    for sym in temp_sym:
      if len(sym.strip())>0:
          list_sym.append(sym.strip())
    # Remove 'none' from symthoms
    if "none" in list_sym:
       list_sym.remove("none");
    if len(list_sym)==0:
        continue
    temp = list()
    for sym in list_sym:
      sym=sym.replace('-',' ') # changed sym.sym to sym
      sym=sym.replace("'",'') # changed sym.sym to sym
      sym = sym.replace('(', '')  # changed sym.sym to sym
      sym = sym.replace(')', '')  # changed sym.sym to sym
      # Use english_stopwords in the list comprehension
      sym = ' '.join([lemmatizer.lemmatize(word) for word in splitter.tokenize(sym) if word not in english_stopwords and not word[0].isdigit()])
      total_symptoms.add(sym)
      temp.append(sym)
    disease_symptoms_cleaned[key] = temp # Fix the typo here. change 'disease_symthoms_cleaned' to 'disease_symptoms_cleaned'

total_symptoms=list(total_symptoms)
total_symptoms.sort()
print(len(total_symptoms))

#store each sympthom's synopnyms as list of words
from itertools import combinations # Import the combinations function
from nltk.corpus import wordnet # import wordnet for synonyms


def get_synonyms(word): # Define the get_synonyms function
    synonyms = []
    for syn in wordnet.synsets(word):
      for lemma in syn.lemmas():
        synonyms.append(lemma.name())
    return synonyms

sym_syn = dict()
for s in total_symptoms:
    symp=s.split()
    str_sym=set()
    for comb in range(1, len(symp)+1):
      for subset in combinations(symp, comb):
          subset=''.join(subset)
          subset = get_synonyms(subset) # call the defined get_synonyms function
          str_sym.update(subset)
    str_sym.add(s)
    str_sym = ' '.join(str_sym).replace('_',' ').lower()
    str_sym = list(set(str_sym.split()))
    str_sym.sort()
    sym_syn[s] = str_sym
#print(s,":",str_sym)
print("Done!")



#for each sympthom pair in dataset, find score of their synonym list.
#jaccard similarity is measure of how similar 2 set are based on the items present in both the sets
#thresholding is type of image segementation, where we change the pixels of an image to make the image easier to analyze
#if Jaccard>threshold, it means that those synonym are similar and one of them can be used in place of other
total_symptoms = sorted(total_symptoms, key=len, reverse=True)
symptom_match=dict()
new_symptoms = set()
for i,symi in enumerate(total_symptoms):
    for j in range(i+1, len(total_symptoms)):
        symj=total_symptoms[j]
        syn_symi = set(sym_syn[symi])
        syn_symj = set(sym_syn[symj])
        jaccard = len(syn_symi.intersection(syn_symj))/len(syn_symi.union(syn_symj))
        if jaccard>0.75:
           print(symi,"->",symj)
           # Store similiar symptoms in dictionary, replace symj with symi, so
           # Sympto_match[symj] = symi
           if symi in symptom_match.keys():
               symptom_match[symj] = symptom_match[symi]
           else:
               symptom_match[symj] = symi
new_symptoms = set(total_symptoms).difference(set(symptom_match.keys()))
print(len(new_symptoms))

#print(symptom_match)

# After removing similar symptoms, final lsit of symptoms is stores in new_symptoms
total_symptoms = new_symptoms
total_symptoms = list(total_symptoms)
total_symptoms.sort()
total_symptoms=['label_dis']+total_symptoms

#initialize two dataframes, one for normal dataset and one for combinational dataset
!pip install pandas
import pandas as pd # import the pandas library and give it the alias 'pd'
df_comb = pd.DataFrame(columns=total_symptoms)
df_norm = pd.DataFrame(columns=total_symptoms)

#read each disease and symptom list, convert into dictionary and add to dataframe
diseases_symptoms_cleaned = {'disease1': ['symptom1', 'symptom2'], 'disease2': ['symptom3']} # Define the dictionary
for key, values in diseases_symptoms_cleaned.items():
    key = str.encode(key).decode('utf-8')
    tmp = []

#for similar symptoms, replace with the value in dictionary
for symptom in values:
  if symptom in symptom_match.keys():
      tmp.append(symptom_match[symptom])
      # print(symptom)
  else:
      tmp.append(symptom)

values = list(set(tmp))
diseases_symptoms_cleaned[key] = values

# Populate row for normal
row_norm = dict({x:0 for x in total_symptoms})
for sym in values:
    row_norm[sym] = 1
row_norm['label_dis']=key
df_norm = pd.concat([df_norm, pd.DataFrame([row_norm])], ignore_index=True)

# Popular rows for combination dataset
for comb in range(1, len(values) + 1):
    for subset in combinations(values, comb):
        row_comb = dict({x:0 for x in total_symptoms})
        for sym in list(subset):
            row_comb[sym]=1
        row_comb['label_dis']=key
        df_comb = pd.concat([df_comb,pd.DataFrame([row_comb])], ignore_index=True)